{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezahamzeh69/Intrusion-Detection-LSTM/blob/main/unsw_nb15_ids_bilstm_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1ssGXgdXy6r",
        "outputId": "fef6be31-ebed-428a-e24e-0deb94749969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading UNSW-NB15 dataset using kagglehub...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dhoogla/unswnb15?dataset_version_number=5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11.7M/11.7M [00:00<00:00, 127MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/dhoogla/unswnb15/versions/5\n",
            "Attempting to load training data from: /root/.cache/kagglehub/datasets/dhoogla/unswnb15/versions/5/UNSW_NB15_training-set.parquet\n",
            "Attempting to load testing data from: /root/.cache/kagglehub/datasets/dhoogla/unswnb15/versions/5/UNSW_NB15_testing-set.parquet\n",
            "Datasets loaded successfully.\n",
            "Preprocessing datasets...\n",
            "Preprocessing finished. Input dimension: 188\n",
            "Creating sequences...\n",
            "Raw training sequences: 175327\n",
            "Raw testing sequences: 82318\n",
            "Splitting 15.0% of training sequences for validation.\n",
            "Final Train sequences: 149027\n",
            "Validation sequences: 26300\n",
            "Final Test sequences: 82318\n",
            "Using class weights for CrossEntropyLoss: [2.101370e-05 9.858141e-06]\n",
            "\n",
            "--- Starting Training on cuda ---\n",
            "Epoch 1/30 | Train Loss: 0.1508 | Val Loss: 0.0906 | Val Acc: 0.9681 | Val F1: 0.9767 | Time: 13.20s\n",
            "Epoch 2/30 | Train Loss: 0.0930 | Val Loss: 0.0803 | Val Acc: 0.9676 | Val F1: 0.9760 | Time: 12.13s\n",
            "Epoch 3/30 | Train Loss: 0.0836 | Val Loss: 0.0888 | Val Acc: 0.9734 | Val F1: 0.9807 | Time: 12.26s\n",
            "Epoch 4/30 | Train Loss: 0.0785 | Val Loss: 0.0776 | Val Acc: 0.9742 | Val F1: 0.9811 | Time: 12.31s\n",
            "Epoch 5/30 | Train Loss: 0.0741 | Val Loss: 0.0898 | Val Acc: 0.9742 | Val F1: 0.9813 | Time: 12.47s\n",
            "Epoch 6/30 | Train Loss: 0.0714 | Val Loss: 0.0722 | Val Acc: 0.9736 | Val F1: 0.9805 | Time: 12.56s\n",
            "Epoch 7/30 | Train Loss: 0.0690 | Val Loss: 0.0677 | Val Acc: 0.9741 | Val F1: 0.9808 | Time: 12.93s\n",
            "Epoch 8/30 | Train Loss: 0.0674 | Val Loss: 0.0679 | Val Acc: 0.9710 | Val F1: 0.9785 | Time: 12.86s\n",
            "Epoch 9/30 | Train Loss: 0.0666 | Val Loss: 0.0803 | Val Acc: 0.9584 | Val F1: 0.9687 | Time: 12.86s\n",
            "Epoch 10/30 | Train Loss: 0.0641 | Val Loss: 0.0693 | Val Acc: 0.9757 | Val F1: 0.9821 | Time: 12.97s\n",
            "Epoch 11/30 | Train Loss: 0.0611 | Val Loss: 0.0708 | Val Acc: 0.9776 | Val F1: 0.9836 | Time: 13.36s\n",
            "Epoch 12/30 | Train Loss: 0.0595 | Val Loss: 0.0659 | Val Acc: 0.9786 | Val F1: 0.9843 | Time: 13.15s\n",
            "Epoch 13/30 | Train Loss: 0.0568 | Val Loss: 0.0588 | Val Acc: 0.9767 | Val F1: 0.9828 | Time: 12.94s\n",
            "Epoch 14/30 | Train Loss: 0.0551 | Val Loss: 0.0694 | Val Acc: 0.9795 | Val F1: 0.9850 | Time: 12.91s\n",
            "Epoch 15/30 | Train Loss: 0.0535 | Val Loss: 0.0582 | Val Acc: 0.9770 | Val F1: 0.9830 | Time: 12.91s\n",
            "Epoch 16/30 | Train Loss: 0.0514 | Val Loss: 0.0590 | Val Acc: 0.9760 | Val F1: 0.9823 | Time: 12.98s\n",
            "Epoch 17/30 | Train Loss: 0.0495 | Val Loss: 0.0540 | Val Acc: 0.9785 | Val F1: 0.9841 | Time: 12.89s\n",
            "Epoch 18/30 | Train Loss: 0.0478 | Val Loss: 0.0574 | Val Acc: 0.9774 | Val F1: 0.9833 | Time: 12.98s\n",
            "Epoch 19/30 | Train Loss: 0.0460 | Val Loss: 0.0573 | Val Acc: 0.9778 | Val F1: 0.9836 | Time: 12.92s\n",
            "Epoch 20/30 | Train Loss: 0.0438 | Val Loss: 0.0634 | Val Acc: 0.9716 | Val F1: 0.9788 | Time: 12.94s\n",
            "Epoch 21/30 | Train Loss: 0.0439 | Val Loss: 0.0616 | Val Acc: 0.9695 | Val F1: 0.9772 | Time: 12.97s\n",
            "Epoch 22/30 | Train Loss: 0.0416 | Val Loss: 0.0645 | Val Acc: 0.9783 | Val F1: 0.9840 | Time: 12.91s\n",
            "Early stopping triggered after epoch 22!\n",
            "--- Training Finished --- Total time: 4m 43s\n",
            "Loaded best model weights for final evaluation.\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "Test Loss: 0.0313\n",
            "Test Accuracy: 0.9891\n",
            "Test F1 Score (Binary): 0.9900\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36703   283]\n",
            " [  617 44715]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       0.98      0.99      0.99     36986\n",
            "  Attack (1)       0.99      0.99      0.99     45332\n",
            "\n",
            "    accuracy                           0.99     82318\n",
            "   macro avg       0.99      0.99      0.99     82318\n",
            "weighted avg       0.99      0.99      0.99     82318\n",
            "\n",
            "--------------------------------\n",
            "\n",
            "Execution finished successfully.\n"
          ]
        }
      ],
      "source": [
        "# !pip install pyarrow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import kagglehub\n",
        "import io\n",
        "\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "LSTM_LAYERS = 2\n",
        "TRANSFORMER_DIM = 128\n",
        "NHEAD = 8\n",
        "NUM_TRANSFORMER_LAYERS = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT = 0.3\n",
        "SEQUENCE_LENGTH = 15\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 30\n",
        "VALIDATION_SPLIT_FROM_TRAIN = 0.15\n",
        "RANDOM_SEED = 42\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "TRAIN_FILE_NAME = 'UNSW_NB15_training-set.parquet'\n",
        "TEST_FILE_NAME = 'UNSW_NB15_testing-set.parquet'\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_and_preprocess_unsw(dataset_dir):\n",
        "    train_file_path = os.path.join(dataset_dir, TRAIN_FILE_NAME)\n",
        "    test_file_path = os.path.join(dataset_dir, TEST_FILE_NAME)\n",
        "    print(f\"Attempting to load training data from: {train_file_path}\")\n",
        "    print(f\"Attempting to load testing data from: {test_file_path}\")\n",
        "    if not os.path.exists(train_file_path) or not os.path.exists(test_file_path):\n",
        "        print(\"Error: Training or Testing file not found in the downloaded dataset directory.\")\n",
        "        print(f\"Contents of {dataset_dir}: {os.listdir(dataset_dir)}\")\n",
        "        return None, None, None, None, -1\n",
        "    try:\n",
        "        df_train = pd.read_parquet(train_file_path)\n",
        "        df_test = pd.read_parquet(test_file_path)\n",
        "        print(\"Datasets loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Parquet files: {e}\")\n",
        "        return None, None, None, None, -1\n",
        "    print(\"Preprocessing datasets...\")\n",
        "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "    df = df.drop(['id', 'attack_cat'], axis=1, errors='ignore')\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace(r'[^a-z0-9_]', '', regex=True)\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    for col in numerical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(df[col].median() if df[col].nunique() > 10 else 0)\n",
        "    for col in categorical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            if pd.api.types.is_categorical_dtype(df[col]):\n",
        "                df[col] = df[col].astype('object')\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "    if 'label' in numerical_cols:\n",
        "        numerical_cols.remove('label')\n",
        "    elif 'label' in categorical_cols:\n",
        "        categorical_cols.remove('label')\n",
        "        try:\n",
        "            df['label'] = pd.to_numeric(df['label'])\n",
        "        except ValueError:\n",
        "            print(\"Error: Could not convert 'label' column to numeric.\")\n",
        "            return None, None, None, None, -1\n",
        "    if 'label' not in df.columns or not pd.api.types.is_numeric_dtype(df['label']):\n",
        "        print(\"Error: 'label' column is missing or not numeric after cleaning.\")\n",
        "        return None, None, None, None, -1\n",
        "    df_encoded = pd.get_dummies(df, columns=categorical_cols, dummy_na=False)\n",
        "    train_len = len(df_train)\n",
        "    df_train_processed = df_encoded.iloc[:train_len].copy()\n",
        "    df_test_processed = df_encoded.iloc[train_len:].copy()\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_cols_encoded = df_train_processed.drop('label', axis=1).select_dtypes(include=np.number).columns.tolist()\n",
        "    if numerical_cols_encoded:\n",
        "        for col in numerical_cols_encoded:\n",
        "            df_train_processed[col] = df_train_processed[col].astype(np.float32)\n",
        "            df_test_processed[col] = df_test_processed[col].astype(np.float32)\n",
        "        scaler.fit(df_train_processed[numerical_cols_encoded])\n",
        "        df_train_processed.loc[:, numerical_cols_encoded] = scaler.transform(df_train_processed[numerical_cols_encoded])\n",
        "        df_test_processed.loc[:, numerical_cols_encoded] = scaler.transform(df_test_processed[numerical_cols_encoded])\n",
        "    feature_cols = df_train_processed.columns.drop('label')\n",
        "    try:\n",
        "        X_train = df_train_processed[feature_cols].astype(np.float32).values\n",
        "        X_test = df_test_processed[feature_cols].astype(np.float32).values\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting feature columns to float32 before .values: {e}\")\n",
        "        for col in feature_cols:\n",
        "            try:\n",
        "                df_train_processed[col].astype(np.float32)\n",
        "            except Exception as col_e:\n",
        "                print(f\"  - Column '{col}' failed conversion: {col_e}, dtype: {df_train_processed[col].dtype}\")\n",
        "        return None, None, None, None, -1\n",
        "    y_train = df_train_processed['label'].astype(np.int64).values\n",
        "    y_test = df_test_processed['label'].astype(np.int64).values\n",
        "    input_dim = X_train.shape[1]\n",
        "    print(f\"Preprocessing finished. Input dimension: {input_dim}\")\n",
        "    return X_train, y_train, X_test, y_test, input_dim\n",
        "\n",
        "def create_sequences(features, labels, sequence_length):\n",
        "    sequences = []\n",
        "    sequence_labels = []\n",
        "    if len(features) < sequence_length:\n",
        "        return np.array([]), np.array([])\n",
        "    for i in range(len(features) - sequence_length + 1):\n",
        "        sequences.append(features[i:i+sequence_length])\n",
        "        sequence_labels.append(labels[i+sequence_length-1])\n",
        "    return np.array(sequences, dtype=np.float32), np.array(sequence_labels, dtype=np.int64)\n",
        "\n",
        "class BiLSTMTransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, lstm_hidden_dim, lstm_layers, transformer_dim, nhead, num_transformer_layers, num_classes, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=dropout if lstm_layers > 1 else 0, bidirectional=True)\n",
        "        self.fc_lstm_transformer = nn.Linear(lstm_hidden_dim * 2, transformer_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=nhead, dim_feedforward=transformer_dim * 4, dropout=dropout, activation='gelu', batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(transformer_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        transformer_input = self.fc_lstm_transformer(lstm_out)\n",
        "        transformer_out = self.transformer_encoder(transformer_input)\n",
        "        pooled = self.global_pool(transformer_out.transpose(1, 2)).squeeze(-1)\n",
        "        dropped_out = self.dropout_layer(pooled)\n",
        "        logits = self.classifier(dropped_out)\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device, is_test_set=False):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    if not data_loader or len(data_loader.dataset) == 0:\n",
        "        if not is_test_set:\n",
        "            return np.nan, np.nan, np.nan\n",
        "        print(\"Warning: Cannot evaluate on empty or invalid dataloader.\")\n",
        "        return 0.0, 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in data_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            total_loss += loss.item() * batch_data.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_predictions += (predicted == batch_labels).sum().item()\n",
        "            all_labels.extend(batch_labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = correct_predictions / len(data_loader.dataset)\n",
        "    f1 = 0.0\n",
        "    if len(all_labels) > 0 and len(all_predictions) > 0:\n",
        "        f1 = f1_score(all_labels, all_predictions, average='binary', zero_division=0)\n",
        "    if is_test_set:\n",
        "        print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "        print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Test F1 Score (Binary): {f1:.4f}\")\n",
        "        if len(all_labels) > 0 and len(all_predictions) > 0:\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(confusion_matrix(all_labels, all_predictions))\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(all_labels, all_predictions, target_names=['Normal (0)', 'Attack (1)'], zero_division=0))\n",
        "        else:\n",
        "            print(\"\\nNo labels/predictions available for Confusion Matrix/Classification Report.\")\n",
        "        print(\"--------------------------------\")\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience):\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_score': []}\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    total_start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * batch_data.size(0)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        if val_loader:\n",
        "            val_loss, val_accuracy, val_f1_score = evaluate_model(model, val_loader, criterion, device)\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_accuracy'].append(val_accuracy)\n",
        "            history['val_f1_score'].append(val_f1_score)\n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1_score:.4f} | Time: {epoch_duration:.2f}s\")\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                epochs_no_improve = 0\n",
        "                torch.save(model.state_dict(), 'best_bilstm_transformer_model.pth')\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= patience:\n",
        "                    print(f\"Early stopping triggered after epoch {epoch+1}!\")\n",
        "                    break\n",
        "        else:\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(np.nan)\n",
        "            history['val_accuracy'].append(np.nan)\n",
        "            history['val_f1_score'].append(np.nan)\n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Time: {epoch_duration:.2f}s\")\n",
        "    total_training_time = time.time() - total_start_time\n",
        "    print(f\"--- Training Finished --- Total time: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
        "    if val_loader and os.path.exists('best_bilstm_transformer_model.pth'):\n",
        "        try:\n",
        "            model.load_state_dict(torch.load('best_bilstm_transformer_model.pth', map_location=device))\n",
        "            print(\"Loaded best model weights for final evaluation.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load best model weights ({e}). Using last epoch model.\")\n",
        "    elif not val_loader:\n",
        "        torch.save(model.state_dict(), 'last_epoch_bilstm_transformer_model.pth')\n",
        "        print(\"Saved model from last epoch (no validation performed).\")\n",
        "    return history\n",
        "\n",
        "print(\"Downloading UNSW-NB15 dataset using kagglehub...\")\n",
        "try:\n",
        "    dataset_path = kagglehub.dataset_download(\"dhoogla/unswnb15\")\n",
        "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Please ensure Kaggle API credentials are set up correctly in your environment.\")\n",
        "    dataset_path = None\n",
        "\n",
        "if dataset_path and os.path.isdir(dataset_path):\n",
        "    X_train_raw, y_train_raw, X_test_raw, y_test_raw, input_dim = load_and_preprocess_unsw(dataset_path)\n",
        "    if X_train_raw is not None and input_dim != -1:\n",
        "        INPUT_DIM = input_dim\n",
        "        print(\"Creating sequences...\")\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train_raw, y_train_raw, SEQUENCE_LENGTH)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test_raw, y_test_raw, SEQUENCE_LENGTH)\n",
        "        if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
        "            print(\"Error: Not enough data to create sequences with the specified length.\")\n",
        "        else:\n",
        "            print(f\"Raw training sequences: {len(X_train_seq)}\")\n",
        "            print(f\"Raw testing sequences: {len(X_test_seq)}\")\n",
        "            X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = [], [], [], []\n",
        "            if len(X_train_seq) > 1 and VALIDATION_SPLIT_FROM_TRAIN > 0:\n",
        "                try:\n",
        "                    X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = train_test_split(\n",
        "                        X_train_seq, y_train_seq, test_size=VALIDATION_SPLIT_FROM_TRAIN, random_state=RANDOM_SEED, stratify=y_train_seq\n",
        "                    )\n",
        "                    print(f\"Splitting {VALIDATION_SPLIT_FROM_TRAIN*100:.1f}% of training sequences for validation.\")\n",
        "                except ValueError as e:\n",
        "                    print(f\"Warning: Could not stratify split ({e}). Performing non-stratified split.\")\n",
        "                    X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = train_test_split(\n",
        "                        X_train_seq, y_train_seq, test_size=VALIDATION_SPLIT_FROM_TRAIN, random_state=RANDOM_SEED\n",
        "                    )\n",
        "                print(f\"Final Train sequences: {len(X_train_final_seq)}\")\n",
        "                print(f\"Validation sequences: {len(X_val_seq)}\")\n",
        "                print(f\"Final Test sequences: {len(X_test_seq)}\")\n",
        "            elif len(X_train_seq) > 0:\n",
        "                print(\"Using all training sequences for training, no validation split performed.\")\n",
        "                X_train_final_seq, y_train_final_seq = X_train_seq, y_train_seq\n",
        "                X_val_seq, y_val_seq = np.array([]), np.array([])\n",
        "                print(f\"Final Train sequences: {len(X_train_final_seq)}\")\n",
        "                print(f\"Validation sequences: 0\")\n",
        "                print(f\"Final Test sequences: {len(X_test_seq)}\")\n",
        "            else:\n",
        "                print(\"Error: No training sequences available.\")\n",
        "                X_train_final_seq = []\n",
        "            if len(X_train_final_seq) > 0:\n",
        "                X_train_tensor = torch.from_numpy(X_train_final_seq)\n",
        "                y_train_tensor = torch.from_numpy(y_train_final_seq)\n",
        "                X_val_tensor = torch.from_numpy(X_val_seq)\n",
        "                y_val_tensor = torch.from_numpy(y_val_seq)\n",
        "                X_test_tensor = torch.from_numpy(X_test_seq)\n",
        "                y_test_tensor = torch.from_numpy(y_test_seq)\n",
        "                num_workers = 2 if DEVICE.type == 'cuda' else 0\n",
        "                pin_memory_flag = True if DEVICE.type == 'cuda' else False\n",
        "                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "                val_dataset = TensorDataset(X_val_tensor, y_val_tensor) if len(X_val_seq) > 0 else None\n",
        "                test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory_flag, drop_last=True)\n",
        "                val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory_flag) if val_dataset else None\n",
        "                test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n",
        "                model = BiLSTMTransformerModel(INPUT_DIM, LSTM_HIDDEN_DIM, LSTM_LAYERS, TRANSFORMER_DIM, NHEAD, NUM_TRANSFORMER_LAYERS, NUM_CLASSES, DROPOUT).to(DEVICE)\n",
        "                if len(y_train_final_seq) > 0:\n",
        "                    class_counts = np.bincount(y_train_final_seq)\n",
        "                    if len(class_counts) == NUM_CLASSES and 0 not in class_counts:\n",
        "                        class_weights = torch.tensor([1.0 / c for c in class_counts], dtype=torch.float32).to(DEVICE)\n",
        "                        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "                        print(f\"Using class weights for CrossEntropyLoss: {class_weights.cpu().numpy()}\")\n",
        "                    else:\n",
        "                        print(\"Using standard CrossEntropyLoss (no weighting or class count issue).\")\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "                else:\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "                history = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, DEVICE, EARLY_STOPPING_PATIENCE)\n",
        "                evaluate_model(model, test_loader, criterion, DEVICE, is_test_set=True)\n",
        "                print(\"\\nExecution finished successfully.\")\n",
        "            else:\n",
        "                print(\"\\nExecution aborted: No training data available after sequencing/splitting.\")\n",
        "    else:\n",
        "        print(\"\\nExecution aborted due to data loading/preprocessing errors.\")\n",
        "else:\n",
        "    print(\"\\nExecution aborted: Dataset download failed or directory not found.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKTXf5YqIU+XNnomHTNL7o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}